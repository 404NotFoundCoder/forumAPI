from openai import OpenAI

# deployé–‹
from api.vector_search import vector_search_light

# localé–‹
# from vector_search import (
#     vector_search_light,
# )


# token = os.environ["GITHUB_TOKEN"]
ENDPOINT = "https://models.inference.ai.azure.com"
MODEL_NAME = "gpt-4o-mini"


V_SENPAI_SYSTEM_PROMPT = """
ä½ æ˜¯ã€ŒV-Senpaiã€ï¼Œä¸€ä½å…·å‚™è±å¯Œç¶“é©—çš„å­¸é•·å§Šæ¨¡æ“¬æ©Ÿå™¨äººã€‚ä½ çš„ä»»å‹™æ˜¯å”åŠ©å­¸ç”Ÿäº†è§£è¼”ä»å¤§å­¸è³‡ç®¡ç³»ã€Œç³»çµ±åˆ†æèˆ‡è¨­è¨ˆã€èª²ç¨‹ï¼ˆåˆç¨± SAã€å°å°ˆé¡Œï¼‰èˆ‡ã€Œå°ˆé¡Œå¯¦ä½œã€ä¹‹é–“çš„å·®ç•°èˆ‡æ­·å±†ç¶“é©—ã€‚
ä½ æœƒæ ¹æ“šæ­·å±†å­¸ç”Ÿçš„è¨ªè«‡ç´€éŒ„èˆ‡èª²ç¨‹èƒŒæ™¯çŸ¥è­˜ï¼Œæ‰®æ¼”ä¸€ä½ä¸­æ–‡èª²å ‚åŠ©æ•™ï¼Œå¹«åŠ©å­¸ç”Ÿé‡æ¸…å›°æƒ‘ã€æä¾›å»ºè­°èˆ‡ç¶“é©—åˆ†äº«ã€‚
è«‹åš´æ ¼éµå®ˆä»¥ä¸‹è¦å‰‡ï¼š
1. **è³‡æ–™ç‚ºæœ¬ï¼Œç¦æ­¢çŒœæ¸¬æˆ–æé€ è³‡è¨Šã€‚**  
   - å›ç­”åªèƒ½æ ¹æ“šè³‡æ–™ä¸­å‡ºç¾çš„å…§å®¹ï¼ˆä¾‹å¦‚ï¼šè¨ªè«‡ã€èª²ç¨‹è¦åŠƒç­‰ï¼‰ã€‚  
   - è‹¥æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè«‹èªªï¼šã€Œæˆ‘æ‰¾ä¸åˆ°ç›¸é—œè³‡æ–™ã€ï¼Œä¸¦é¼“å‹µå­¸ç”Ÿæ”¹å•å…¶ä»–è§’åº¦ã€‚  
2. **å•é¡Œæ¨¡ç³Šæ™‚ï¼Œå”åŠ©é‡æ¸…å†å›ç­”ã€‚**  
   - è‹¥å­¸ç”Ÿå•é¡Œä¸æ¸…æ¥šï¼Œè«‹ä¸»å‹•åˆ—å‡ºé¸é …æˆ–è¿½å•ï¼Œå”åŠ©å°æ–¹èšç„¦ã€‚  
3. **å›ç­”æ–¹å¼è¦å…·é«”ã€çœŸèª ã€æœ‰æ¢ç†ã€‚**  
   - èˆ‰ä¾‹æ™‚è«‹æŒ‡å‡ºæ˜¯ä¾†è‡ªã€ŒæŸä½åŒå­¸çš„ç¶“é©—ã€ã€‚  
   - ä¸è¦ä½¿ç”¨éæ–¼ç©ºæ³›çš„å»ºè­°ï¼Œä¾‹å¦‚ã€Œå¤šåŠªåŠ›ã€ã€ã€ŒåŠ æ²¹å°±å¥½ã€é€™é¡ç„¡å¯¦è³ªå¹«åŠ©çš„å›ç­”ã€‚  
4. **ä»¥ä¸­æ–‡ä½œç­”ã€‚**  
   - å›ç­”è¦å£èªã€è‡ªç„¶ã€ç°¡æ½”æ˜ç¢ºã€‚
"""


def get_vector_search_result(user_input: str) -> dict:
    """åªé€²è¡Œå‘é‡æœå°‹ï¼Œä¸å›å‚³ LLM å›æ‡‰"""
    search_result = vector_search_light(user_input)
    print("ğŸ” å‘é‡æœå°‹çµæœ:", search_result)

    return {
        "sources": search_result.get("sources", []),
        "ids": search_result.get("ids", []),
        "matches": search_result.get("matches", []),
        "context_text": search_result.get("text", "æŸ¥ç„¡è³‡æ–™ã€‚"),
    }


def get_openai_response(token: str, user_input: str, context_text: str = None) -> str:
    client = OpenAI(
        base_url=ENDPOINT,
        api_key=token,
    )

    # å¦‚æœæ²’æœ‰æä¾› context_textï¼Œå‰‡é‡æ–°æœå°‹
    if context_text is None:
        search_result = vector_search_light(user_input)
        context_text = search_result.get("text", "æŸ¥ç„¡è³‡æ–™ã€‚")
    messages = [
        {
            "role": "system",
            "content": V_SENPAI_SYSTEM_PROMPT
            + f"\n\nä»¥ä¸‹æ˜¯ä½ å¯ä»¥åƒè€ƒçš„è³‡æ–™ï¼š\n{context_text}",
        },
        {"role": "user", "content": user_input},
    ]

    response = client.chat.completions.create(
        model=MODEL_NAME, messages=messages, temperature=1.0, top_p=1.0
    )

    # print("AAAæ©Ÿå™¨äººæ”¶åˆ°çš„è³‡æ–™",messages)
    print("AAAæ©Ÿå™¨äººå›æ‡‰", response.choices[0].message.content)
    return {
        "answer": response.choices[0].message.content,
    }
